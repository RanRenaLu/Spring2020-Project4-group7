---
title: "Project4"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

In this project, we are going to explore matrix factorization methods for recommender system. The goal is to match consumers with most appropriate products. Matrix factorization methods characterize both items and users by vectors of factors inferred from item rating patterns. High correspondence between item and user factors leads to a recommendation. 

### Step 1 Load Data and Train-test Split
```{r warning=FALSE, message=FALSE}
library(MASS)
library(dplyr)
library(tidyr)
library(ggplot2)
data <- read.csv("../data/ml-latest-small/ratings.csv")
set.seed(0)
test_idx <- sample(1:nrow(data), round(nrow(data)/5, 0))
train_idx <- setdiff(1:nrow(data), test_idx)
data_train <- data[train_idx,]
data_test <- data[test_idx,]
```

###Step 2 Matrix Factorization
#### Step 2.1 Algorithm
Here I perform probabilistic gradien descent to do matrix factorization.

A2. [Gradient Descent with Probabilistic Assumptions](./paper/P3 probabilistic-matrix-factorization.pdf) Section 2

```{r}
U <- length(unique(data$userId))
I <- length(unique(data$movieId))
source("../lib/Matrix_Factorization_A2.R")
```


#### Step 2.2 Parameter Tuning
Here you should tune parameters, such as the dimension of factor and the penalty parameter $\lambda$ by cross-validation.
```{r}
source("../lib/cross_validation_A2.R")
f_l <- cbind(f = c(5, 10, 20, 5, 10, 20, 5, 10, 20),
             sigma_p = c(1, 1, 1, 0.5, 0.5, 0.5, 1.5, 1.5, 1.5),
             sigma_q = c(1, 1, 1, 0.5, 0.5, 0.5, 1.5, 1.5, 1.5))
```

DO NOT RUN THIS CHUNK!!!
```{r, eval=FALSE}
result_summary <- array(NA, dim = c(nrow(f_l), 10, 4)) 

run_time <- system.time(for(i in 1:nrow(f_l)){
    par <- paste("f = ", f_l[i,1], ", lambda = ", 10^f_l[i,2])
    cat(par, "\n")
    current_result <- cv.function.pmf(data, K = 5, f = f_l[i,1],sigma_p=f_l[i,2],sigma_q= f_l[i,3],sigma = 0.1)
    result_summary[i,,] <- matrix(unlist(current_result), ncol = 4, byrow = T) 
    print(result_summary)
  
})
result_summary[,,1]
save(result_summary, file = "../output/rmse_pmf.Rdata")
```

```{r}
load("../output/rmse_pmf.Rdata")
rmse <- data.frame(rbind(result_summary[,,1], result_summary[,,2]), train_test = rep(c("Train", "Test"), each = 9), par = rep(paste("f = ", f_l[,1], ", sigma_p = ", f_l[,2], ", sigma_q = ", f_l[,3]), times = 2)) %>% gather("epoch", "RMSE", -train_test, -par)
rmse$epoch <- as.numeric(gsub("X", "", rmse$epoch))
rmse %>% ggplot(aes(x = epoch, y = RMSE, col = train_test)) + geom_point() + facet_grid(~par)
```

#### Step 2.3 Evaluation on the Model without Postprocessing 
```{r, eval= FALSE}
time <- system.time(result <- gradesc_pmf(f = 5, sigma_p = 0.5,sigma_q = 0.5,lrate = 0.01, max.iter = 100, stopping.deriv = 0.01, data = data, train = data_train, test = data_test))

save(result, file = "../output/mat_fac_pmf.RData")
```

You should visualize training and testing RMSE by different epochs ([One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9)). 

(Not yet by now)
```{r}
load(file = "../output/mat_fac_pmf.RData")
library(ggplot2)

RMSE <- data.frame(epochs = seq(10, 100, 10), Training_MSE = result$train_RMSE, Test_MSE = result$test_RMSE) %>% gather(key = train_or_test, value = RMSE, -epochs)

RMSE %>% ggplot(aes(x = epochs, y = RMSE,col = train_or_test)) + geom_point() + scale_x_discrete(limits = seq(10, 100, 10)) + xlim(c(0, 100))

```
  
```{r}

cat("The traing RMSE of the Probabilistic Gradient Descent model without postprocessing is", RMSE[10,3], "\n")
cat("The test RMSE of the Probabilistic Gradient Descent model without postprocessing is", RMSE[20,3], "\n")
```  